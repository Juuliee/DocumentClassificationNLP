{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification On Imbalanced Data\n",
    "------------------\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "__[1. Introduction](#first-bullet)__\n",
    "\n",
    "__[2. Preprocessing & Feature Extraction](#second-bullet)__\n",
    "\n",
    "__[3. The Naive Bayes Model](#third-bullet)__\n",
    "\n",
    "__[4. Scikit-learn Pipelines](#fourth-bullet)__\n",
    "\n",
    "__[5. Stop Word Removal](#fifth-bullet)__\n",
    "\n",
    "__[6. Handling Imbalanced Data](#sixth-bullet)__\n",
    "\n",
    "__[7. HyperParameter Tunning With GridSearchCV](#seventh-bullet)__\n",
    "\n",
    "__[7. Conclusions](#eigth-bullet)__\n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "-----------\n",
    "\n",
    "Natural language processing or NLP is an hot topic in data science and machine learning.  While research in NLP dates back to the 1950's, the real revolution in this domain came in 1980's and 1990's with the introduction of statistical models and fast computational power. Before this most language processing tasks made use of hand-coded rules which were generally not very robust.\n",
    "\n",
    "The span of topics in Natural Language Processing is immense and I'll just getting to the tip of the iceberg with going over the topic of document classification.  I will be working the <a href=\"http://scikit-learn.org/\">Scikit-learn</a> library and using its provided dataset, the <a href=\"http://qwone.com/~jason/20Newsgroups/\">20 News Groups</a>, which is collection of almost 20,000 articles on 20 different topics or 'newsgroups'. We can obtain the training and testing sets directly with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test  = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then view the total number of articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data) + len(twenty_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the documents within the data set belong to the following 20 topics,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the actual message within the documents,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = \"\\n\".join(twenty_train.data[0].split(\"\\n\"))\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we look at the target classes they are actually encoded using a categorical variable,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to view the human readible version of the target we create a dictionary to map the categorical variables (numbers) to labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings = list(range(len(twenty_train.target_names)))\n",
    "target_dic = dict(zip(codings,twenty_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the actual document's topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.autos'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dic[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alt.atheism',\n",
       " 1: 'comp.graphics',\n",
       " 2: 'comp.os.ms-windows.misc',\n",
       " 3: 'comp.sys.ibm.pc.hardware',\n",
       " 4: 'comp.sys.mac.hardware',\n",
       " 5: 'comp.windows.x',\n",
       " 6: 'misc.forsale',\n",
       " 7: 'rec.autos',\n",
       " 8: 'rec.motorcycles',\n",
       " 9: 'rec.sport.baseball',\n",
       " 10: 'rec.sport.hockey',\n",
       " 11: 'sci.crypt',\n",
       " 12: 'sci.electronics',\n",
       " 13: 'sci.med',\n",
       " 14: 'sci.space',\n",
       " 15: 'soc.religion.christian',\n",
       " 16: 'talk.politics.guns',\n",
       " 17: 'talk.politics.mideast',\n",
       " 18: 'talk.politics.misc',\n",
       " 19: 'talk.religion.misc'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One piece of termanology that is used frequently in NLP is the text <a href=\"https://en.wikipedia.org/wiki/Text_corpus\">**corpus**</a>. The text corpus is a large and structured set of texts and can be used to describe what the `twent_train.data` and `twenty_test.data` datasets are.\n",
    "\n",
    "Now that we have an idea of what kind of data we are working with we can start to do some machine learning on it.  As with all datasets there is some required preprocessing before machine learning.  With numerical data there needs to be some cleaning and <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html\">scaling</a> of the features. However, in Natural Language Processing there is much more substantial preprocessing stage that we'll go over next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "train_df = pd.DataFrame({\"text\": twenty_train.data,\n",
    "                         \"target\": twenty_train.target})\n",
    "\n",
    "test_df = pd.DataFrame({\"text\": twenty_test.data,\n",
    "                        \"target\": twenty_test.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df  = train_df[train_df[\"target\"].isin([7,8,9,10])]\n",
    "test_df   = test_df[test_df[\"target\"].isin([7,8,9,10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "7     594\n",
       "8     598\n",
       "9     597\n",
       "10    600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(\"target\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "7     396\n",
       "8     398\n",
       "9     397\n",
       "10    399\n",
       "dtype: int64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby(\"target\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hockey_df     = train_df[train_df[\"target\"] == 10].sample(frac=0.25)\n",
    "train_no_hockey_df  = train_df[train_df[\"target\"] != 10]\n",
    "\n",
    "test_hockey_df     = test_df[test_df[\"target\"] == 10].sample(frac=0.25)\n",
    "test_no_hockey_df  = test_df[test_df[\"target\"] != 10]\n",
    "\n",
    "imbal_train_df = pd.concat([train_hockey_df, train_no_hockey_df],axis=0)\n",
    "imbal_test_df  = pd.concat([test_no_hockey_df, test_hockey_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = [\"rec.autos\", \"rec.motorcycles\",\"rec.sport.baseball\",\"rec.sport.hockey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "7     594\n",
       "8     598\n",
       "9     597\n",
       "10    150\n",
       "dtype: int64"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imbal_train_df.groupby(\"target\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "7     30.634348\n",
       "8     30.840640\n",
       "9     30.789067\n",
       "10     7.735946\n",
       "dtype: float64"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * imbal_train_df.groupby(\"target\").size() / imbal_train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "7     30.673896\n",
       "8     30.828815\n",
       "9     30.751356\n",
       "10     7.745933\n",
       "dtype: float64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * imbal_test_df.groupby(\"target\").size() / imbal_test_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing & Feature Extraction <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to go over is the concept of the <a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\">**bag of words model**</a>.  *In the bag-of-words model, a text (such as a sentence or a document) is represented as \"bag\" or list of its words, disregarding grammar and even word order, but keeping multiplicity of the words.*  A two document example is:\n",
    "\n",
    "- **D1:**  Hi, I am Mike and I like Boston.\n",
    "\n",
    "- **D2:**  Boston is a city and people in Boston like the Red Sox.\n",
    "\n",
    "From these two documents, a list, or 'bag-of-words' is constructed\n",
    "\n",
    "    bag = ['Hi', 'I', 'am', 'Mike', 'and', 'like', 'Boston', 'is', \n",
    "           'a', 'city, 'and', 'people', 'in', 'the', 'red', 'sox]\n",
    "\n",
    "\n",
    "Notice how in our bag-of-words we have dropped repetitions of the words 'I', 'is' and 'Mike', we will show how multiplicity of words enters into our model next. \n",
    "\n",
    "The bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a \"bag of words\", we can calculate various measures to characterize the document.  In order to do so we have to generate a vector for each document that represents the number of times each entry in the bag of words appears in the text. The order of entries in the vector corresponds to the order of the entries in the bag-of-words list.  For example, document D1 would have a vector,\n",
    "\n",
    "    [1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0 ,0, 0, 0, 0, 0]\n",
    "    \n",
    "while the second document, D2, would have the vector,\n",
    "\n",
    "    [0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "Each entry of the lists refers to frequency or count of the corresponding entry in the bag-of-words list.  When we have a stacked collection of (row) vectors, or matrix, where each row corresponds to a document (vector), and each column corresponds to a word in the bag-of-words list, then this will be known as our **term-frequency ($\\text{tf}$) [document matrix](https://en.wikipedia.org/wiki/Document-term_matrix)**. The general formula for an entry in the $\\text{tf}$ matrix is,\n",
    "\n",
    "$$\\text{tf}(d,t) \\,  = \\, f_{t,d}$$\n",
    "    \n",
    "where $f_{t,d}$ is the number of times the term $t$ occurs in document $d \\in \\mathcal{D}$, where $\\mathcal{D}$ is our text corpus.  We can create a term-frequency matrix using Scikit-learns <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\">CountVectorizer</a> class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of term-frequency matrix: (1939, 27138)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_tf = count_vect.fit_transform(imbal_train_df[\"text\"])\n",
    "\n",
    "print(\"Shape of term-frequency matrix:\", X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term-frequency is a sparse matrix where each row is a document in our training corpus ($\\mathcal{D}$) and each column corresponds to a term/word in the bag-of-words list. This can be confirmed by comparing the number of rows in the term-frequency matrix to the number of documents in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training documents:  1939\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training documents: \", imbal_train_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most often term-frequency alone is not a good measure of the importance of a word/term to a document's topic.  Very common words like \"the\", \"a\", \"to\" are almost always the terms with the highest frequency in the text. Thus, having a high raw count of the number of times a term appears in a document does not necessarily mean that the corresponding word is more important. Furtermore, longer documents could have high frequency of terms that do not correlate with the document topic, but instead occur with high numbers solely due to the length of the document.\n",
    "\n",
    "To circumvent the limination of term-frequency, we often normalize it by the **inverse document frequency (idf)**.  This results in the **term frequency-inverse document frequency (tf-idf)** matrix.  The *inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents in the corpus*.  We can give a formal defintion of the inverse-document-frequency by letting $\\mathcal{D}$ be the corpus or the set of all documents and $N$ is the number of documents in the corpus and $N_{t,D}$ be the number of documents that contain the term $t$ then, \n",
    "\n",
    "$$idf(t,\\mathcal{D}) \\, = \\,  \\log\\left(\\frac{N_{\\mathcal{D}}}{1 + N_{t,\\mathcal{D}}}\\right) \\, = \\, -  \\log\\left(\\frac{1 + N_{t,\\mathcal{D}}}{N_{\\mathcal{D}}}\\right) $$\n",
    "\n",
    "The reason for the presence of the $1$ is for smoothing.  Without it, if the term/word did not appear in any training documents, then its inverse-document-frequency would be $idf(t,\\mathcal{D}) = \\infty$.  However, with the presense of the $1$ it will now have $idf(t,\\mathcal{D}) = 0$.\n",
    "\n",
    "\n",
    "Now we can formally defined the term frequnecy-inverse document frequency as a normalized version of term-frequency,\n",
    "\n",
    "\n",
    "$$\\text{tf-idf}(t,d) \\, = \\, tf(t,d) \\cdot idf(t,\\mathcal{D}) $$\n",
    "\n",
    "Like the term-frequency, the term frequency-inverse document frequency is a sparse matrix, where again, each row is a document in our training corpus ($\\mathcal{D}$) and each column corresponds to a term/word in the bag-of-words list.  The $\\text{tf-idf}$ matrix can be constructed using the sklearn <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">TfidfTransformer</a> class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1939, 27138)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_tf)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that the $\\text{tf-idf}$ matrix is the same shape as the $\\text{tf}$ matri, but the two have different values. \n",
    "\n",
    "\n",
    "Now that we have built our $\\text{tf-idf}$ matrix we can start to look at the which terms/words are most associated with document topics and then build a predictive model to classify the documents' topic. Let's first go over the model we will be using for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Bayes Model <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "-------------------\n",
    "\n",
    "One of the most basic models for text classification is the <a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\">Naive Bayes model</a>. The Naive Bayes classification model predicts the document topic, $y = \\{C_{1},C_{2},\\ldots, C_{20}\\}$ where $C_{k}$ is the class or topic based on the document feactures $\\textbf{x} \\in \\mathbb{N}^{p}$,  and $p$ is the number of terms in our bag-of-words list.  The feature vector,\n",
    "\n",
    "$$\\textbf{x} \\, = \\, \\left[ x_{1}, x_{2}, \\ldots , x_{p} \\right] $$\n",
    "\n",
    "contains counts $x_{i}$ for the $\\text{tf-idf}$ value of the i-th term in our bag-of-words list.  Using <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\">Bayes Theorem</a> we can develop a model to predict the topic class  ($C_{k}$) of a document from its feature vector $\\textbf{x}$,\n",
    "\n",
    "$$P\\left(C_{k} \\, \\vert \\, x_{1}, \\ldots , x_{p} \\right) \\; = \\; \\frac{P\\left(x_{1}, \\ldots, x_{p} \\, \\vert \\, C_{k} \\right)P(C_{k})}{P\\left(x_{1}, \\ldots, x_{p} \\right)}$$\n",
    "\n",
    "The Naive Bayes model makes the \"Naive\" assumption the probability of each term's $\\text{tf-idf}$ is **conditionally independent** of every other term.  This reduces our **conditional probability function** to the product,\n",
    "\n",
    "$$ P\\left(x_{1}, \\ldots, x_{p} \\, \\vert \\, C_{k} \\right) \\; = \\; \\Pi_{i=1}^{p} P\\left(x_{i} \\, \\vert \\, C_{k} \\right)$$\n",
    "\n",
    "Subsequently Bayes' theorem for our classification problem becomes,\n",
    "\n",
    "$$P\\left(C_{k} \\, \\vert \\, x_{1}, \\ldots , x_{p} \\right) \\; = \\; \\frac{ P(C_{k}) \\, \\Pi_{i=1}^{p} P\\left(x_{i} \\, \\vert \\, C_{k} \\right)}{P\\left(x_{1}, \\ldots, x_{p} \\right)}$$\n",
    "\n",
    "\n",
    "Since the denominator is independent of the class ($C_{k}$) we can use a <a href=\"https://en.wikipedia.org/wiki/Maximum_a_posteriori\">Maxmimum A Posteriori</a> method to estimate the document topic , \n",
    "\n",
    "$$ \\hat{y} \\, = \\, \\text{arg max}_{k}\\;  P(C_{k}) \\,  \\Pi_{i=1}^{p} P\\left(x_{i} \\, \\vert \\, C_{k} \\right)$$ \n",
    "\n",
    "\n",
    "The **prior**, $P(C_{k}),$ is often taken to be the relative frequency of the class in the training corpus, while the form of the conditional distribution $P\\left(x_{i} \\, \\vert \\, C_{k} \\right)$ is a choice of the modeler and determines the type of Naive Bayes classifier. \n",
    "\n",
    "\n",
    "We will use a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\">multinomial Naive Bayes</a> model which works well when our features are discrete variables such as those in our $\\text{tf-idf}$ matrix.  In the multinomial Naive Bayes model the conditional probability takes the form,\n",
    "\n",
    "\n",
    "$$ P\\left(x_{1}, \\ldots, x_{p} \\, \\vert \\, C_{k} \\right) \\, = \\, \\frac{\\left(\\sum_{i=1}^{p} x_{i}\\right)!}{\\Pi_{i=1}^{p} x_{i}!}  \\Pi_{i=1}^{p} p_{k,i}^{x_{i}}$$\n",
    "\n",
    "\n",
    "where $p_{k,i}$ is the probability that the $k$-th class will have the $i$-th bag-of-words term in its feature vector. This leads to our **posterior distribution** having the functional form,\n",
    "\n",
    "$$P\\left(C_{k} \\, \\vert \\, x_{1}, \\ldots , x_{p} \\right) \\; = \\; \\frac{ P(C_{k})}{P\\left(x_{1}, \\ldots, x_{p} \\right)} \\, \\frac{\\left(\\sum_{i=1}^{p} x_{i}\\right)!}{\\Pi_{i=1}^{p} x_{i}!}  \\Pi_{i=1}^{p} p_{k,i}^{x_{i}}$$\n",
    "\n",
    "The Naive Bayes classifier can be fast compared to more sophisticated methods due to the decoupling of the class conditional feature distributions, i.e.\n",
    "\n",
    "$$ P\\left(x_{1}, \\ldots, x_{p} \\, \\vert \\, C_{k} \\right) \\; = \\; \\Pi_{i=1}^{p} P\\left(x_{i} \\, \\vert \\, C_{k} \\right)$$\n",
    "\n",
    "The decoupling of the class conditional distributions allows for each distribution to be independently estimated as a one dimensional distribution and helps to alleviate problems with the curse of dimensionality.\n",
    "\n",
    "We can instantiate a multinomial Naive Bayes classifier using the Scikit-learn library and fit it to our  $\\text{tf-idf}$ matrix using the commands,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mod = MultinomialNB()\n",
    "mod.fit(X_train_tfidf, imbal_train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term `alpha=1` means we are using <a href=\"https://en.wikipedia.org/wiki/Laplace_smoothing\">Laplace smoothing</a>. We can now look at the accuracy of our classifier using Scikit-learns <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">accuracy_score</a> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8985282726568552\n"
     ]
    }
   ],
   "source": [
    "X_test_tf = count_vect.transform(imbal_test_df[\"text\"])\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_tf)\n",
    "\n",
    "predicted = mod.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(imbal_test_df[\"target\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the topics within the training corpus are pretty well balanced the accuracy is pretty representative of the performance of our model.  However, we can get more a detailed view of the performance of our classifier by using the Scikit-learn library's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\">classification report</a> function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.94      0.98      0.96       396\n",
      "   rec.motorcycles       0.97      0.96      0.96       398\n",
      "rec.sport.baseball       0.81      0.98      0.89       397\n",
      "  rec.sport.hockey       0.00      0.00      0.00       100\n",
      "\n",
      "          accuracy                           0.90      1291\n",
      "         macro avg       0.68      0.73      0.70      1291\n",
      "      weighted avg       0.84      0.90      0.86      1291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mukeharmon/miniconda/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(imbal_test_df[\"target\"],\n",
    "                            predicted, \n",
    "                            target_names=new_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the average precision is around 82%, but the average recall and f1-score are 77%. We can try to improve this using some more advanced preprocessing techniques.  This will be made much easier by using the concept of Scikit-learn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">pipeline</a> utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn Pipelines <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "---------------\n",
    "In Scitkit-learn pipelines are a sequence of transforms followed by a final estimator. Intermediate steps within the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The <code>CountVectorizer</code> and <code>TfidfTransformer</code> are used as transformers in our above example.  The final estimator of a pipeline only needs to implement the fit method.  We can see the simplicity of pipelines by using it to re-implement our above analysis using the Naive Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8985282726568552\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.94      0.98      0.96       396\n",
      "   rec.motorcycles       0.97      0.96      0.96       398\n",
      "rec.sport.baseball       0.81      0.98      0.89       397\n",
      "  rec.sport.hockey       0.00      0.00      0.00       100\n",
      "\n",
      "          accuracy                           0.90      1291\n",
      "         macro avg       0.68      0.73      0.70      1291\n",
      "      weighted avg       0.84      0.90      0.86      1291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mukeharmon/miniconda/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', MultinomialNB()),])\n",
    "\n",
    "mod = pipe.fit(imbal_train_df[\"text\"], imbal_train_df[\"target\"])\n",
    "\n",
    "predicted = mod.predict(imbal_test_df[\"text\"])\n",
    "print(\"Accuracy:\", accuracy_score(imbal_test_df[\"target\"], predicted))\n",
    "print()\n",
    "print(classification_report(imbal_test_df[\"target\"],\n",
    "                            predicted, \n",
    "                            target_names=new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.90      0.98      0.94       396\n",
      "   rec.motorcycles       0.93      0.96      0.94       398\n",
      "rec.sport.baseball       0.52      0.98      0.68       397\n",
      "  rec.sport.hockey       1.00      0.02      0.03       399\n",
      "\n",
      "          accuracy                           0.73      1590\n",
      "         macro avg       0.84      0.73      0.65      1590\n",
      "      weighted avg       0.84      0.73      0.65      1590\n",
      "\n",
      "\n",
      "Accuracy: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', MultinomialNB()),])\n",
    "\n",
    "mod       = pipe.fit(imbal_train_df[\"text\"], imbal_train_df[\"target\"])\n",
    "\n",
    "predicted = mod.predict(test_fin[\"text\"])\n",
    "\n",
    "\n",
    "print(classification_report(test_df[\"target\"],\n",
    "                            predicted, \n",
    "                            target_names=new_labels))\n",
    "print()\n",
    "print(\"Accuracy:\", accuracy_score(test_df[\"target\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice how when using pipelines we passed `twenty_train.data` and `twenty_test.data` into the `fit` and predict methods instead of the `X_train_tfidf` and `X_test_tfidf` objects as we did in the previous section. The transformations occur under-the-hood using the Scikit-learn pipeline functionality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stop Words Removal <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "------------\n",
    "\n",
    "We can look to improve our model by removing <a href=\"https://en.wikipedia.org/wiki/Stop_words\">stop words</a> which are common words in the english language and do not add any information into the text. These includes words such as, \"the\", \"at\", \"is\", etc.  We can remove them in the `CountVectorizer` constructor call,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy)\n",
      "Subject: Re: Flames Truly Brutal in Loss\n",
      "Organization: University of Toronto Chemistry Department\n",
      "Lines: 11\n",
      "\n",
      "In article <vzhivov.735193129@cunews> vzhivov@superior.carleton.ca (Vladimir Zhivov) writes:\n",
      ">As the subject suggests the Flames were not impressive this afternoon,\n",
      ">dropping a 6-3 decision to the LA Kings. Most of the Flames neglected\n",
      ">to show up, especially in their own zone, as the Kings hit at least\n",
      ">five posts! The Flames best line was probably\n",
      "\n",
      "Mike Vernon is now 3 wins 11 losses plus that All-Star game debacle in\n",
      "afternoon games during his career...with another afternoon game with\n",
      "Los Angeles next Sunday...has the ABC deal doomed the Flames?\n",
      "\n",
      "Gerald\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = \"\\n\".join(imbal_train_df[\"text\"].iloc[1].split(\"\\n\"))\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/mukeharmon/miniconda/envs/NLP/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /Users/mukeharmon/miniconda/envs/NLP/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From : golchowy @ alchemy.chem.utoronto.ca ( Gerald Olchowy )\n",
      "Subject : Re : Flames Truly Brutal Loss\n",
      "Organization : University Toronto Chemistry Department\n",
      "Lines : 11\n",
      "\n",
      "In article < vzhivov.735193129 @ cunews > vzhivov @ superior.carleton.ca ( Vladimir Zhivov ) writes :\n",
      "> As subject suggests Flames impressive afternoon ,\n",
      "> dropping 6-3 decision LA Kings . Most Flames neglected\n",
      "> show , especially zone , Kings hit least\n",
      "> five posts ! The Flames best line probably\n",
      "\n",
      "Mike Vernon 3 wins 11 losses plus All-Star game debacle\n",
      "afternoon games career ... another afternoon game\n",
      "Los Angeles next Sunday ... ABC deal doomed Flames ?\n",
      "\n",
      "Gerald\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words    = set(stopwords.words('english')) \n",
    "\n",
    "lines         = imbal_train_df[\"text\"].iloc[1].split(\"\\n\")\n",
    "\n",
    "lines_tokens  = [word_tokenize(line) for line in lines]\n",
    "\n",
    "filter_lines  = [\" \".join([token for token in tokens if token not in stop_words])\n",
    "                 for tokens in lines_tokens]\n",
    "\n",
    "print(\"\\n\".join(filter_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.95      0.97      0.96       396\n",
      "   rec.motorcycles       0.96      0.96      0.96       398\n",
      "rec.sport.baseball       0.81      0.99      0.89       397\n",
      "  rec.sport.hockey       1.00      0.01      0.02       100\n",
      "\n",
      "          accuracy                           0.90      1291\n",
      "         macro avg       0.93      0.73      0.71      1291\n",
      "      weighted avg       0.91      0.90      0.86      1291\n",
      "\n",
      "\n",
      "Accuracy: 0.8985282726568552\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vec',    CountVectorizer(stop_words='english')),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', MultinomialNB())])\n",
    "\n",
    "mod = pipe.fit(imbal_train_df[\"text\"], imbal_train_df[\"target\"])\n",
    "\n",
    "predicted = mod.predict(imbal_test_df[\"text\"])\n",
    "\n",
    "print(classification_report(imbal_test_df[\"target\"],\n",
    "                            predicted, \n",
    "                            target_names=new_labels))\n",
    "print()\n",
    "print(\"Accuracy:\", accuracy_score(imbal_test_df[\"target\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that removing stop words gives us a decent improvement in our recall and f1-score!\n",
    "\n",
    "\n",
    "## Handling Class Imbalance <a class=\"anchor\" id=\"sixth-bullet\"></a>\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9341595662277304\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.95      0.97      0.96       396\n",
      "   rec.motorcycles       0.97      0.96      0.97       398\n",
      "rec.sport.baseball       0.88      0.99      0.93       397\n",
      "  rec.sport.hockey       1.00      0.47      0.64       100\n",
      "\n",
      "          accuracy                           0.93      1291\n",
      "         macro avg       0.95      0.85      0.87      1291\n",
      "      weighted avg       0.94      0.93      0.93      1291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "upsampled_hockey_df = resample(hockey_df,\n",
    "                            replace=True,\n",
    "                            n_samples=300,\n",
    "                            random_state=123)\n",
    "\n",
    "upsampled_df = pd.concat([no_hockey_df, upsampled_hockey_df], axis=0)\n",
    "\n",
    "model        = pipe.fit(upsampled_df[\"text\"], upsampled_df[\"target\"])\n",
    "\n",
    "predicted    = model.predict(imbal_test_df[\"text\"])\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(imbal_test_df[\"target\"], predicted))\n",
    "\n",
    "print(classification_report(imbal_test_df[\"target\"],\n",
    "                            predicted, \n",
    "                            target_names=new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.87      0.97      0.92       396\n",
      "   rec.motorcycles       0.86      0.96      0.91       398\n",
      "rec.sport.baseball       0.87      0.86      0.86       397\n",
      "  rec.sport.hockey       1.00      0.04      0.08       100\n",
      "\n",
      "          accuracy                           0.86      1291\n",
      "         macro avg       0.90      0.71      0.69      1291\n",
      "      weighted avg       0.87      0.86      0.83      1291\n",
      "\n",
      "Accuracy: 0.8628969790859798\n"
     ]
    }
   ],
   "source": [
    "baseball_df = no_hockey_df[no_hockey_df[\"target\"] == 9]\n",
    "others_df   = no_hockey_df[no_hockey_df[\"target\"] != 9]\n",
    "\n",
    "downsamp_base_df = resample(baseball_df,\n",
    "                           replace=True,\n",
    "                           n_samples=400,\n",
    "                           random_state=123)\n",
    "\n",
    "downsampled_df = pd.concat([others_df, hockey_df, downsamp_base_df], axis=0)\n",
    "                          \n",
    "model          = pipe.fit(downsampled_df[\"text\"], downsampled_df[\"target\"])\n",
    "\n",
    "predicted      = model.predict(imbal_test_df[\"text\"])\n",
    "\n",
    "print(classification_report(imbal_test_df[\"target\"],\n",
    "                            predicted, \n",
    "                            target_names=new_labels))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(imbal_test_df[\"target\"], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.88      0.97      0.93       396\n",
      "   rec.motorcycles       0.89      0.96      0.92       398\n",
      "rec.sport.baseball       0.98      0.85      0.91       397\n",
      "  rec.sport.hockey       0.99      0.74      0.85       100\n",
      "\n",
      "          accuracy                           0.92      1291\n",
      "         macro avg       0.93      0.88      0.90      1291\n",
      "      weighted avg       0.92      0.92      0.92      1291\n",
      "\n",
      "Accuracy: 0.9163439194422928\n"
     ]
    }
   ],
   "source": [
    "bothsampled_df = pd.concat([others_df, upsampled_hockey_df, downsamp_base_df], axis=0)\n",
    "\n",
    "mod            = pipe.fit(bothsampled_df[\"text\"], bothsampled_df[\"target\"])\n",
    "\n",
    "predicted      = mod.predict(imbal_test_df[\"text\"])\n",
    "\n",
    "print(classification_report(imbal_test_df[\"target\"],\n",
    "                            predicted, \n",
    "                            target_names=new_labels))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(imbal_test_df[\"target\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HyperParameter Tuning With GridSearchCV <a class=\"anchor\" id=\"seventh-bullet\"></a>\n",
    "----------------\n",
    "\n",
    "Not only do pipelines allow us to swap out our model much easier, (say replace our Naive Bayes classifier with a support vector machine) but they also allow us to assemble several steps that can be cross-validated together while setting different parameters. To do this, pieplines enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'vec__min_df': (1,3),\n",
    "          'tfidf__smooth_idf': (True, False),\n",
    "          'tfidf__use_idf': (True, False),\n",
    "          'model__alpha': (1, 1e-1, 1e-2, 1e-3),\n",
    "          'model__fit_prior': (True,False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how when we wish to change the model parameter `alpha` (which is a smoothing regularizer) by including on \"model\" and not the `mod` object. We can perform the grid search in parallel by setting `n_jobs=-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe, params, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then fit the data to perform the actual grid search,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mukeharmon/miniconda/envs/NLP/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "gs_model = grid_search.fit(bothsampled_df[\"text\"], bothsampled_df[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model `gs_model` uses the paremeters which had the best cross-validated score. We can see a full description of the best model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.96      0.96      0.96       396\n",
      "   rec.motorcycles       0.96      0.97      0.97       398\n",
      "rec.sport.baseball       0.97      0.94      0.96       397\n",
      "  rec.sport.hockey       0.89      0.94      0.91       100\n",
      "\n",
      "          accuracy                           0.96      1291\n",
      "         macro avg       0.95      0.96      0.95      1291\n",
      "      weighted avg       0.96      0.96      0.96      1291\n",
      "\n",
      "Accuracy:  0.9589465530596437\n"
     ]
    }
   ],
   "source": [
    "predicted = gs_model.predict(imbal_test_df[\"text\"])\n",
    "print(classification_report(imbal_test_df[\"target\"],\n",
    "                            predicted, \n",
    "                            target_names=new_labels))\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predicted, imbal_test_df[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('vec',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=1.0,\n",
      "                                 max_features=None, min_df=1,\n",
      "                                 ngram_range=(1, 1), preprocessor=None,\n",
      "                                 stop_words='english', strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, vocabulary=None)),\n",
      "                ('tfidf',\n",
      "                 TfidfTransformer(norm='l2', smooth_idf=False,\n",
      "                                  sublinear_tf=False, use_idf=True)),\n",
      "                ('model',\n",
      "                 MultinomialNB(alpha=0.1, class_prior=None, fit_prior=False))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(gs_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how our model performs on the test set,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mukeharmon/miniconda/envs/NLP/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.96      0.97      0.96       396\n",
      "   rec.motorcycles       0.98      0.97      0.97       398\n",
      "rec.sport.baseball       0.97      0.95      0.96       397\n",
      "  rec.sport.hockey       0.96      0.98      0.97       399\n",
      "\n",
      "          accuracy                           0.97      1590\n",
      "         macro avg       0.97      0.97      0.97      1590\n",
      "      weighted avg       0.97      0.97      0.97      1590\n",
      "\n",
      "Accuracy:  0.9672955974842767\n"
     ]
    }
   ],
   "source": [
    "gs_model_orig = grid_search.fit(train_df[\"text\"], train_df[\"target\"])\n",
    "\n",
    "predicted = gs_model_orig.predict(test_df[\"text\"])\n",
    "print(classification_report(test_df[\"target\"],\n",
    "                            predicted, \n",
    "                            target_names=new_labels))\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predicted, test_df[\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a class=\"anchor\" id=\"eigth-bullet\"></a>\n",
    "\n",
    "\n",
    "In this blog post we covered document classification using Scikit-learn and the 20 News Groups dataset.  We went over the basics of term frequency-inverse document frequency, pipelines and the Naive Bayes classifier.  While we were able to improve the accuracy of the basic Naive Bayes classifier from 77% to 83%, there are much more sophisticated models like a <a href=\"http://scikit-learn.org/stable/modules/svm.html#svm\">support vector machines</a> which can achieve higher performance.\n",
    "\n",
    "One thing that we did not address was the topic of <a href=\"https://en.wikipedia.org/wiki/Stemming\">stemming</a> and <a href=\"https://en.wikipedia.org/wiki/Lemmatisation\">lemmatisation</a>, which both have to do with reducing a word down to its base form and when used can improve the performance of text classification models.  Lemmatisation differs from stemming because it depends on identifying the intended part of speech and meaning of a word in a sentence.  Stemmers and lemmatizers are both provided by Natural Language Tool Kit or <a href=\"http://www.nltk.org/\">NLTK</a> and <a href=\"http://www.nltk.org/\">spaCy</a> libraries, but that will have to wait for another day!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
